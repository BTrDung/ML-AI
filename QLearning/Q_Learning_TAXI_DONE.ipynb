{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning-TAXI-DONE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2pAVQgcPZ0J"
      },
      "source": [
        "#pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUUkeqxsQWAo"
      },
      "source": [
        "import gym "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wSKa_L7QaPN",
        "outputId": "a2cecf82-6da6-4083-c70b-2982e8e6b90b"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\").env\r\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBz4hHfJT5fx"
      },
      "source": [
        "# An action number from 0-5 where:\r\n",
        "# 0 = south\r\n",
        "# 1 = north\r\n",
        "# 2 = east\r\n",
        "# 3 = west\r\n",
        "# 4 = pickup\r\n",
        "# 5 = dropoff"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ8C7xVDRnZY",
        "outputId": "2eb17863-4d51-4d37-d81c-10f0d094eafe"
      },
      "source": [
        "env.reset()  \r\n",
        "env.render()\r\n",
        "\r\n",
        "#print(\"Number of action space {}\".format(env.action_space))\r\n",
        "#print(\"Number of state sSpace {}\".format(env.observation_space))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyOdKP_Yb9S_",
        "outputId": "aaa57d82-9022-4b00-f91b-31f3f6512eda"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\r\n",
        "print(\"State:\", state)\r\n",
        " \r\n",
        "env.s = state\r\n",
        "env.render()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KPzKwjzXxJ1",
        "outputId": "9a1068d3-6ae7-4f31-a53c-8bd7b4f3edf1"
      },
      "source": [
        "env.P[100]\r\n",
        "# action -> probability, nextstate, reward, done"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 200, -1, False)],\n",
              " 1: [(1.0, 0, -1, False)],\n",
              " 2: [(1.0, 120, -1, False)],\n",
              " 3: [(1.0, 100, -1, False)],\n",
              " 4: [(1.0, 100, -10, False)],\n",
              " 5: [(1.0, 100, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV8ADgdBwlwF"
      },
      "source": [
        "Solving without RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piVFD7ZWwkJO",
        "outputId": "f8aa63d3-00a7-4eb8-a203-55d11d1d7e0b"
      },
      "source": [
        "env.s = 328 \r\n",
        "\r\n",
        "epochs = 0\r\n",
        "penalties, reward = 0, 0\r\n",
        "frames = [] \r\n",
        "done = False\r\n",
        "\r\n",
        "while not done:\r\n",
        "    action = env.action_space.sample()\r\n",
        "    state, reward, done, info = env.step(action)\r\n",
        "    if reward == -10:\r\n",
        "        penalties += 1\r\n",
        "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward })\r\n",
        "    epochs += 1\r\n",
        "    \r\n",
        "    \r\n",
        "print(\"Step taken: {}\".format(epochs))\r\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step taken: 86\n",
            "Penalties incurred: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyOM2kP7x9Sb"
      },
      "source": [
        "from IPython.display import clear_output \r\n",
        "from time import sleep \r\n",
        "\r\n",
        "def print_frames(frames): \r\n",
        "    for i, frame in enumerate(frames): \r\n",
        "        clear_output(wait = True) \r\n",
        "        print(frame['frame'])\r\n",
        "        print(f\"Timestep: {i + 1}\")\r\n",
        "        print(f\"State: {frame['state']}\")\r\n",
        "        print(f\"Action: {frame['action']}\")\r\n",
        "        print(f\"Reward: {frame['reward']}\")\r\n",
        "        sleep(0.1) \r\n",
        "\r\n",
        "#print_frames(frames)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWwVlEejM6se"
      },
      "source": [
        "In our Taxi environment, we have the reward table, ```P```, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\r\n",
        "\r\n",
        "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-value are updated using the equation: \r\n",
        "\r\n",
        "$Q(state, action) \\leftarrow (1 - \\alpha)Q(state, action) + \\alpha(reward + \\gamma*max_{a}Q(\\text{next state}, \\text{all actions}))$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_CcxwkPX6ix",
        "outputId": "f9ad194d-f9c9-48ba-e999-29206b203af5"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\r\n",
        "\r\n",
        "print(q_table)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIldusDPYc_B",
        "outputId": "c86b5c62-4d4b-49b9-e479-de06b8e9f8e7"
      },
      "source": [
        "import random \r\n",
        "\r\n",
        "alpha = 0.1 \r\n",
        "gamma = 0.6 \r\n",
        "\r\n",
        "epsilon = 0.1\r\n",
        "\r\n",
        "all_epochs = [] \r\n",
        "all_penalties = [] \r\n",
        "\r\n",
        "for i in range(1, 100000):\r\n",
        "    state = env.reset() \r\n",
        "\r\n",
        "    epochs, penalties, reward = 0, 0, 0 \r\n",
        "    done = False \r\n",
        "\r\n",
        "    while not done:\r\n",
        "        if random.uniform(0, 1) < epsilon: \r\n",
        "            action = env.action_space.sample() \r\n",
        "        else: \r\n",
        "            action = np.argmax(q_table[state])\r\n",
        "\r\n",
        "        next_state, reward, done, info = env.step(action) \r\n",
        "\r\n",
        "        old_value = q_table[state, action]\r\n",
        "        next_max = np.max(q_table[next_state])\r\n",
        "\r\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) \r\n",
        "        q_table[state, action] = new_value \r\n",
        "\r\n",
        "        if reward == -10: \r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        state = next_state \r\n",
        "        epochs += 1\r\n",
        "\r\n",
        "    if i % 100 == 0: \r\n",
        "        clear_output(wait = True) \r\n",
        "        print(f\"Episode: {i}\")\r\n",
        "\r\n",
        "print(\"OK\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 99900\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujci7_ISZyBn",
        "outputId": "a8e849e9-9a04-4588-d5ed-a0c25b60e4b7"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.40604837,  -2.27325184,  -2.40099822,  -2.35938433,\n",
              "       -10.65925005, -10.96163233])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiuAv70HZ3nH",
        "outputId": "694490df-fe35-4020-e5c5-d62baa9ed5d3"
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\r\n",
        "episodes = 10\r\n",
        "\r\n",
        "frame = [] \r\n",
        "min_pen = 9999999999999\r\n",
        "\r\n",
        "for _ in range(episodes):\r\n",
        "    state = env.reset()\r\n",
        "    epochs, penalties, reward = 0, 0, 0\r\n",
        "    \r\n",
        "    done = False\r\n",
        "    \r\n",
        "    ff = []\r\n",
        "    while not done:\r\n",
        "        action = np.argmax(q_table[state])\r\n",
        "        state, reward, done, info = env.step(action)\r\n",
        "           \r\n",
        "        if reward == -10:\r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        ff.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward })\r\n",
        "\r\n",
        "        epochs += 1\r\n",
        "    \r\n",
        "    if min_pen > penalties: \r\n",
        "        min_pen = penalties \r\n",
        "        frame = ff \r\n",
        "\r\n",
        "    total_penalties += penalties\r\n",
        "    total_epochs += epochs\r\n",
        "\r\n",
        "print(f\"Results after {episodes} episodes:\")\r\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\r\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 10 episodes:\n",
            "Average timesteps per episode: 11.5\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ69iCRpvwft"
      },
      "source": [
        "#print_frames(frame)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}